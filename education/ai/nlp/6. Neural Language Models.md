# Definitions (What, How, Why)

1. Feedforward
2. Deep learning
3. Bias term
4. Vector
5. Activation
   1. Sigmoid
   2. Tanh
   3. ReLU
6. Saturation
7. Vanishing gradient
8. Perceptron
9. Decision boundary
10. Linearly separable
11. Feedforward Neural Networks
12. Multi-layer Perceptron
13. Hidden layer
14. Fully-connected layer
15. Vector normalization
16. Back propagation
17. Computation graph
18. Chain rule
19. Dropout
20. Hyperparameter
21. one-hot vector
22. pretraining
23. projection layer

# Formulate

1. The output of a hidden layer $A^l$ given the output of previous layer $A^{l-1}$, given the weight matrix $W$ and bias vector $b$ of the affine layer and activation function $g()$ of the activation layer
2. $\text{softmax}(z_i)$ where $z$ is a vector of dimensionality $d$
