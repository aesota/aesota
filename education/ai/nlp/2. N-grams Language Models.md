#  

## Define (What is/How do you/Why)

1. Language Model
2. n-gram
3. Markov assumption
4. Relative Frequency
5. Log Probabilities
6. Extrinsic evaluation of language models
7. Intrinsic evaluation of language models
8. test set
9. training set
10. development set
11. sparsity problem in language models
12. closed vocabulary
13. out of vocabulary
14. open vocabulary
15. Smoothing
    1. discounting
    2. Laplace smoothing
    3. add-one
    4. discounting
    5. discount
    6. add-k
    7. backoff and interpolation
    8. Kneser-ney smoothing
    9. Modified Kneser-ney
16. Huge Language Models
17. Stupid Backoff
18. Entropy
19. Stationary stochastic process
20. Cross-entropy
21. Perplexity

Write formula for:

1. Conditional probability using count
2. Chain rule of probability
3. Bigram model approximation to conditional probability
4. N-gram model approximation to conditional probability
5. Maximum likelihood estimate n-gram parameter estimation
6. Perplexity
7. Katz backoff
8. Good-turing backoff
9. Interpolated absolute discounting applied to bigrams
10. Interpolated Kneser-Ney
11. Entropy
12. Entropy rate
