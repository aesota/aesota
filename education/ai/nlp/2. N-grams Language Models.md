Define (What is/How do you/Why):
1. Language Model
1. n-gram
1. Markov assumption
1. Relative Frequency
1. Log Probabilities
1. Extrinsic evaluation of language models
1. Intrinsic evaluation of language models
1. test set
1. training set
1. development set
1. sparsity problem in language models
1. closed vocabulary
1. out of vocabulary
1. open vocabulary
1. smoothing
1. discounting
1. Laplace smoothing
1. add-one
1. discounting
1. discount
1. add-k
1. backoff and interpolation
1. Kneser-ney smoothing
1. Modified Kneser-ney
1. Huge Language Models
1. Stupid Backoff
1. Entropy
1. Stationary stochastic process
1. Cross-entropy
1. Perplexity

Write formula for:
1. Conditional probability using count
1. Chain rule of probability
1. Bigram model approximation to conditional probability
1. N-gram model approximation to conditional probability
1. Maximum likelihood estimate n-gram parameter estimation
1. Perplexity
1. Katz backoff
1. Good-turing backoff
1. Interpolated absolute discounting applied to bigrams
1. Interpolated Kneser-Ney
1. Entropy
1. Entropy rate