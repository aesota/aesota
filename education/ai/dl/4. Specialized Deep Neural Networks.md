# Specialized Deep Neural Networks

# Define
1. Convolutional networks
1. Convolution
1. Pooling
1. Kernel
1. Feature map
1. Flipped kernel
1. Cross-correlation
1. Toeplitz matrix
1. Doubly block circulant matrix
1. Sparse interactions
1. Parameter sharing
1. Equivariant representations
1. Sparse interactions
1. Sparse connectivity
1. Sparse weights
1. Tied weights
1. Detector stage
1. Pooling function
1. Max pooling
1. Invariant representation
1. Infinitely strong prior
1. Permutation variant
1. Valid convolution
1. Full convolution
1. Unshared convolution
1. Tiled convolution
1. Structured outputs
1. Data types
1. Efficient convolution
1. Separable kernel
1. Random or Unsupervised features
1. Primary visual cortex
1. Simple cells
1. Complex cells
1. Fovea
1. Saccades
1. Time-delay neural networks
1. Reverse correlation
1. Gabor functions
1. Quadrature pair
1. Sequence modeling
1. Recurrent neural networks
1. Unfolding computational graphs
1. Back-propagation through time
1. Teacher forcing
1. Output recurrence
1. Closed-loop
1. RNN gradient
1. Directed graphical models
1. Sequences conditioned on context
1. Bidireectional RNNs
1. Encoder-Decoder Sequence-to-Sequence architectures
1. Deep recurrent networks
1. Recursive neural networks
1. Echo state networks
1. Liquid state machines
1. Reservoir computing
1. Spectral radius
1. Leaky units
1. Skip connections
1. Spectrum of time scales
1. Removing connections
1. Gated RNNs
1. Long short-term memory
1. Gated recurrent unit
1. Forget gate
1. External input gate
1. Output gate
1. Long-term dependencies
1. Clipping gradients
1. Information flow
1. Explicit memory
1. Working memory
1. Memory networks
1. Neural turing machine
1. Content-based addressing
1. Location-based addressing
1. Attention mechanism
1. Linear factor models
1. Probabilistic PCA
1. Factor analysis
1. Conditionally independent
1. Reconstruction error
1. Independent component analysis
1. Nonlinear independent components estimation
1. Slow feature analysis
1. Sparse coding
1. Manifold interpretation of PCA
1. Autoencoder
1. Undercomplete autoencoders
1. Regularized autoencoders
1. Sparse autoencoders
1. Denoising autoencoders
1. Contractive autoencoders
1. Representational power
1. Layer size
1. Depth
1. Stochastic encoders and decoders
1. Denoising autoencoders
1. Reconstruction distribution
1. Denoising score matching
1. Manifold learning with autoencoders
1. Tangent planes
1. Data representation
1. Nonparametric methods
1. Nearest neeighbor graph
1. Predictive sparse decomposition
1. Information retrieval with autoencoders
1. Semantic hashing with autoencoders
1. Representation Learning
1. Unsupervised pretraining
1. Greedy layer-wise unsupervised pretraining
1. Transfer learning
1. Domain adaptation
1. Concept drift
1. One-shot learning
1. Zero-shot learning
1. Zero-data learning
1. Multimodal learning
1. Semi-supervised disentanglingof causal factors
1. Distributed representation
1. Sum-product network
1. Structured probabilistic models
1. Graphical models
1. Density estimation
1. Denoising
1. Missing value imputation
1. Sampling
1. Directed graphical model
1. Belief network
1. Bayesian network
1. Local conditional probability distributions
1. Undirected models
1. Markov random fields
1. Markov networks
1. Factor
1. Clique potential
1. Unnormalized probability distribution
1. Partition function
1. Energy-based models
1. Energy function
1. Boltzmann distribution
1. Boltzmann maachines
1. Product of experts
1. Harmony
1. Free energy
1. Separation
1. D-separation
1. Context-specific independences
1. V-structure
1. Collider case
1. Explaining away effect
1. Immorality
1. Moralized graph
1. Loop
1. Chord
1. Chordal graph
1. Triangulated graph
1. Factor graphs
1. Ancestral sampling
1. Gibbs sampling
1. Structure learning
1. Inference
1. Approximate inference
1. Restricted Boltzmann Machine
1. Block Gibbs Sampling
1. Monte carlo methods
1. Sampling
1. Monte carlo sampling
1. Law of large numbers
1. Central limit theorem
1. Importance sampling
1. Biased importance sampling
1. Markov chain monte carlo methods
1. Ancestral sampling
1. Stochastic matrices
1. Stationary distribution
1. Equilibrium distribution
1. Harris chain
1. Burning in
1. Mixing time
1. Tempering
1. Temperature
1. Tempered transitions
1. Parallel tempering
1. Partition function
1. Log-likelihood gradient
1. Positive phase
1. Negative phase
1. Stochastic maximum likelihood
1. Contrastive divergence
1. Stochastic maximum likelihood
1. Persistent contrastive divergence
1. Fast PCD
1. Pseudolikelihood
1. Generalized pseudolikelihood estimator
1. Score matching
1. Ratio matching
1. Generalized score matching
1. Ratio matching
1. Denoising score matching
1. Noise-contrastive estimation
1. Noise distribution
1. Self-contrastive estimation
1. Partition function
1. Annealed importance sampling
1. Bridge sampling
1. Linked importance sampling
1. Approximate inference
1. Semi-restricted Boltzmann Machine
1. Evidence lower bound
1. Negative variational free energy
1. Expectation Maximization
1. E-step
1. M-step
1. Maximum a posteriori (MAP) inference
1. Variational Inference and Learning
1. Mean field approach
1. Structured variational inference
1. Discrete Latent Variables
1. Binary sparse coding model
1. Damping
1. Calculus of variations
1. Functional derivatives
1. Variational derivatives
1. Euler-lagrange equation
1. Continuous latent variables
1. Interactions between learning and inference
1. Leearned approximate inference
1. Wake-sleep
1. Learned inference
1. Deep generative models
1. Boltzmann machines
1. Boltzmann machine learning
1. Restricted Boltzmann Machines
1. Conditional distributions
1. Deep belief networks
1. Deep boltzmann machines
1. DBM mean field inference
1. DBM parameter learning
1. Layer-wise pretraining
1. Jointly trained deeep boltzmann machines
1. Centered DBM
1. Multi-prediction DBM
1. Gaussian-Bernoulli RBMs
1. Undirected models of conditional covariance
1. Mean and covariance RBM
1. Mean Product of Student t-distributions
1. Spike and Slab RBMs
1. Convolutional Boltzmann Machines
1. Probabilistic max pooling
1. Boltzmann machines for structured or sequential outputs
1. Back-propagation through random operations
1. Back-propagation through discrete stochastic operations
1. Variance reduction
1. Directed generative nets
1. Sigmoid belief networks
1. Differentiable generator networks
1. Inverse transform sampling
1. Variational Autoencoders
1. Deep recurrent attention writer
1. Importance-weighted autoencoder
1. Generative Adversarial Networks
1. Discriminative network
1. Deep Convolutioonal GAN (DCGAN)
1. Self-supervised boosting
1. Generative moment matching networks
1. Maximum mean discrepancy
1. Convolutional generative networks
1. Auto-regressive networks
1. Fully-visible Bayes networks
1. Linear Auto-regressive networks
1. Neural auto-regressive networks
1. Neural auto-regressive density estimator (NADE)
1. Sampling from Autoencoders
1. Generalized denoising autoencoders
1. Clamping
1. Conditional sampling
1. Walk-back training procedure
1. Generative stochastic networks
1. Discriminant GSNs
1. Diffusion inversion
